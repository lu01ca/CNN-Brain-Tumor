---
title: "Classificazione di tumori cerebrali mediante reti neurali convoluzionali"
author: "Luca Iaria"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: true
    toc_float: true
    number_sections: true
  pdf_document:
    toc: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

# Introduzione

Il presente lavoro si pone l’obiettivo di sviluppare un sistema di classificazione per immagini di risonanza magnetica (MRI) cerebrale, finalizzato al riconoscimento di tre tipologie di tumori cerebrali: meningioma, glioma e tumore pituitario.

Il dataset utilizzato è stato originariamente pubblicato da Jun Cheng (2017) ed è reperibile sulla piattaforma Kaggle (https://www.kaggle.com/datasets/denizkavi1/brain-tumor). Dal punto di vista metodologico, l'analisi confronta le performance di quattro diverse reti neurali convoluzionali: due architetture definite e addestrate da zero e due modelli basati su VGG16, implementati tramite tecniche di transfer learning.

# Dataset

## Descrizione

Il dataset utilizzato proviene da Kaggle e contiene 3064 immagini MRI di tumori cerebrali suddivise in tre classi:

- **Meningioma (1)**: tumore che origina dalle meningi (708 immagini);
- **Glioma (2)**: tumore che origina dalle cellule gliali (1426 immagini);
- **Pituitary tumor (3)**: tumore della ghiandola pituitaria (930 immagini).

Come si può osservare, le classi presentano uno sbilanciamento significativo: i gliomi sono circa il doppio dei meningiomi. Questo problema verrà affrontato nel Modello 2 attraverso l'utilizzo di class weights.

## Caricamento delle librerie

```{r libraries}
library(keras)
library(tensorflow)
library(magick)
library(caret)
library(here)
```

## Preprocessing e split
Il dataset è stato ripartito in tre sottoinsiemi (training, validation e test) adottando un campionamento stratificato per mantenere inalterate le proporzioni tra le classi. La suddivisione è stata impostata rispettivamente al 70%, 15% e 15%.

Preliminarmente alla generazione delle directory, è stata implementata una procedura di controllo dell'integrità dei file per identificare ed escludere eventuali immagini corrotte che avrebbero potuto compromettere l'addestramento. Tale analisi non ha rilevato anomalie; pertanto, l'intero dataset originale è stato mantenuto.

```{r paths}
# Percorso delle immagini originali
path <- here("Brain")

# Percorso dove verrà creato il dataset diviso
path.new.dir <- here("Brain split")

# Definizione proporzioni split (stratificato per classe)
prop.train <- 0.70  # 70% Training
prop.val   <- 0.15  # 15% Validation
prop.test  <- 0.15  # 15% Test
```

```{r cleaning-functions}
# Funzione per la pulizia delle immagini corrotte
clean.dir <- function(path.dir) {
  files <- list.files(path.dir, full.names = TRUE, recursive = TRUE)
  count <- 0
  
  for (f in files) {
    info <- file.info(f)
    if (info$size == 0) {
      unlink(f); count <- count + 1; next
    }
    tryCatch({
      img <- image_read(f)
    }, error = function(e) {
      unlink(f); count <- count + 1
    })
  }
  return(count)
}

# Funzione per creare le cartelle e suddividere i file
split.dataset <- function() {
  
  if(!dir.exists(path.new.dir)) dir.create(path.new.dir)
  
  for (set in c("train", "validation", "test")) {
    dir.create(file.path(path.new.dir, set), showWarnings = FALSE)
    for (type in c("meningioma", "glioma", "pituitary tumor")) {
      dir.create(file.path(path.new.dir, set, type), showWarnings = FALSE)
    }
  }
  
  copy <- function(src_name, dest.name) {
    source.folder <- file.path(path, src_name)
    fnames <- list.files(source.folder, pattern = "\\.(jpg|jpeg|png)$", full.names = FALSE)
    n.total <- length(fnames)
    
    set.seed(123)
    fnames <- sample(fnames)
    
    train.idx <- round(n.total * prop.train)
    val.idx   <- round(n.total * prop.val)
    
    train.files <- fnames[1:train.idx]
    val.files   <- fnames[(train.idx + 1):(train.idx + val.idx)]
    test.files  <- fnames[(train.idx + val.idx + 1):n.total]
    
    file.copy(file.path(source.folder, train.files), 
              file.path(path.new.dir, "train", dest.name))
    file.copy(file.path(source.folder, val.files), 
              file.path(path.new.dir, "validation", dest.name))
    file.copy(file.path(source.folder, test.files), 
              file.path(path.new.dir, "test", dest.name))
  }
  
  copy("1", "meningioma")
  copy("2", "glioma")
  copy("3", "pituitary tumor")
  
  clean.dir(file.path(path.new.dir, "train"))
  clean.dir(file.path(path.new.dir, "validation"))
  clean.dir(file.path(path.new.dir, "test"))
}
```

```{r split-data, eval=FALSE}
# Da eseguire solo una volta
split.dataset()
```

```{r class-distribution}
all.files <- list.files(path.new.dir, recursive = TRUE)
(counts <- table(dirname(all.files)))
```
Come si può osservare, anche dopo la suddivisione le classi presentano uno sbilanciamento significativo.
Di seguito viene mostrato un esempio di scansione MRI per ciascuna delle tre classi diagnostiche estratto dal training set.
```{r visualizzazione-immagini, fig.cap="Esempi di scansioni MRI: Meningioma (sx), Glioma (centro), Pituitary tumor (dx)", fig.align='center'}
# Visualizzazione di un campione per classe
img.men <- image_read(list.files(file.path(path.new.dir, "train/meningioma"), 
                                 full.names = TRUE)[1])
img.gli <- image_read(list.files(file.path(path.new.dir, "train/glioma"), 
                                 full.names = TRUE)[1])
img.pit <- image_read(list.files(file.path(path.new.dir, "train/pituitary tumor"), 
                                 full.names = TRUE)[1])

image_append(c(image_scale(img.men), 
               image_scale(img.gli), 
               image_scale(img.pit)))
```

## Data generators
Per evitare di caricare tutte le immagini in RAM contemporaneamente, si utilizzano i data generators di Keras che leggono le immagini dal disco in batch progressivi durante il training. Le immagini vengono ridimensionate a 150×150 pixel e i valori dei pixel normalizzati nell'intervallo [0,1] tramite rescaling (1/255).
La modalità di output è 'categorical' (one-hot encoding) per la classificazione multiclasse, con batch size di 20 immagini.

Per il test set lo shuffling è disattivato per mantenere la corrispondenza tra predizioni e etichette vere.
```{r generators}
train.datagen <- image_data_generator(rescale = 1/255)
val.datagen   <- image_data_generator(rescale = 1/255)
test.datagen  <- image_data_generator(rescale = 1/255)

train.generator <- flow_images_from_directory(
  file.path(path.new.dir, "train"),
  train.datagen,
  target_size = c(150, 150),
  batch_size = 20,
  class_mode = "categorical"
)

validation.generator <- flow_images_from_directory(
  file.path(path.new.dir, "validation"),
  val.datagen,
  target_size = c(150, 150),
  batch_size = 20,
  class_mode = "categorical"
)

test.generator <- flow_images_from_directory(
  file.path(path.new.dir, "test"),
  test.datagen,
  target_size = c(150, 150),
  batch_size = 20,
  class_mode = "categorical",
  shuffle = FALSE
)
```

# Metodologia

In questo progetto vengono confrontati quattro approcci di complessità crescente:

1. **CNN semplice**: architettura base senza regolarizzazione e class weights;
2. **CNN regolarizzata**: architettura con data augmentation, dropout e class weights;
3. **Transfer learning (feature extraction)**: utilizzo di VGG16 pre-addestrato con feature extraction;
4. **Transfer learning (fine-tuning)**: utilizzo di VGG16 pre-addestrato con fine tuning;

## Modello 1: CNN Semplice
L'architettura è composta da quattro blocchi convoluzionali seguiti da un classificatore fully-connected. Ogni blocco convoluzionale applica filtri di dimensione 3×3 seguiti da max pooling 2×2, riducendo progressivamente la dimensionalità spaziale dell'immagine ed estraendo features di complessità crescente: i primi layer catturano pattern di basso livello (bordi, gradienti), mentre i layer più profondi riconoscono strutture più astratte e specifiche del dominio.
La funzione di attivazione ReLU introduce la non-linearità necessaria per apprendere relazioni complesse. Il classificatore finale appiattisce le feature maps in un vettore, le elabora attraverso uno strato denso da 512 unità e produce le probabilità per le tre classi tramite softmax. Il numero totale di parametri è circa 3,45 milioni.

### Architettura

```{r model1-architecture}
model <- keras_model_sequential() %>%
  layer_conv_2d(filters = 32, kernel_size = c(3, 3), activation = "relu", 
                input_shape = c(150, 150, 3)) %>%
  layer_max_pooling_2d(pool_size = c(2, 2)) %>%
  
  layer_conv_2d(filters = 64, kernel_size = c(3, 3), activation = "relu") %>%
  layer_max_pooling_2d(pool_size = c(2, 2)) %>%
  
  layer_conv_2d(filters = 128, kernel_size = c(3, 3), activation = "relu") %>%
  layer_max_pooling_2d(pool_size = c(2, 2)) %>%
  
  layer_conv_2d(filters = 128, kernel_size = c(3, 3), activation = "relu") %>%
  layer_max_pooling_2d(pool_size = c(2, 2)) %>%
  
  layer_flatten() %>%
  layer_dense(units = 512, activation = "relu") %>%
  layer_dense(units = 3, activation = "softmax")

summary(model)
```

### Compilazione e training
Il modello viene compilato con categorical cross-entropy come funzione di perdita, ottimizzatore RMSprop con learning rate $\lambda = 10^{-4}$ e accuracy come metrica di valutazione.

```{r model1-compile}
model %>% compile(
  loss = "categorical_crossentropy",
  # Ottimizzatore per Mac M1
  optimizer = tf$keras$optimizers$legacy$RMSprop(learning_rate = 1e-4),
  metrics = c("acc")
)
```
Il numero di step per epoca viene calcolato come rapporto tra il numero di immagini e la dimensione del batch, garantendo che ogni immagine venga vista esattamente una volta per epoca.
```{r model1-train, eval=FALSE}
steps.train <- ceiling(train.generator$n / train.generator$batch_size)
steps.val   <- ceiling(validation.generator$n / validation.generator$batch_size)

history <- model %>% fit(
  train.generator,
  steps_per_epoch = steps.train,
  epochs = 30,
  validation_data = validation.generator,
  validation_steps = steps.val
)
```

```{r model1-load, include=FALSE}
path_model <- here("Modelli salvati", "modello1.h5")
path_hist  <- here("Modelli salvati", "training1.rds")

model <- load_model_hdf5(path_model)
history <- readRDS(path_hist)
```

### Valutazione

```{r model1-plot}
plot(history)
```

Le curve di apprendimento evidenziano un marcato overfitting: la training loss converge verso zero mentre la validation loss si stabilizza attorno a 0.25, segnalando che il modello ha memorizzato i dati di training senza generalizzare. Questo comportamento è atteso dato il campione relativamente contenuto e l'assenza di tecniche di regolarizzazione.

```{r model1-eval}
model %>% evaluate(test.generator)

prob <- model %>% predict(test.generator)
predict <- apply(prob, 1, which.max) - 1 
true <- test.generator$classes
confusionMatrix(as.factor(predict), as.factor(true))
```
Nonostante l'overfitting, l'accuracy sul test set raggiunge il 94.1%. L'analisi della matrice di confusione rivela però una criticità: la sensitivity per i meningiomi (85.9%) è sensibilmente inferiore rispetto a gliomi (96.3%) e tumori pituitari (97.1%). Questo squilibrio riflette il problema delle classi sbilanciate, il modello tende a favorire la classe maggioritaria (glioma) a scapito della classe minoritaria (meningioma).

## Modello 2: CNN con regolarizzazione

Per contrastare l'overfitting e lo sbilanciamento delle classi, vengono introdotte tre tecniche:

- **Data augmentation**: trasformazioni casuali delle immagini per ampliare virtualmente il training set;
- **Dropout**: regolarizzazione che disattiva casualmente una frazione dei neuroni durante il training;
- **Class Weights**: pesi nella loss function che penalizzano maggiormente gli errori sulle classi minoritarie.

### Class Weights
Per ciascuna classe $j$ si definisce il seguente peso:
$$w_j=\frac{N}{k \times N_j}$$
dove $N$ è il numero totale di immagini, $k$ il numero di classi e $N_j$ il numero di immagini della classe $j$.
```{r model2-weights}
n.glioma <- counts["train/glioma"]
n.men    <- counts["train/meningioma"]
n.pit    <- counts["train/pituitary tumor"]
n.tot <- as.numeric(n.glioma + n.men + n.pit)

class.weights <- list(
  "0" = n.tot / (3 * n.glioma), 
  "1" = n.tot / (3 * n.men), 
  "2" = n.tot / (3 * n.pit)
) 
```

### Data augmentation
Con dataset di dimensioni limitate, la data augmentation permette di espandere artificialmente il training set applicando trasformazioni geometriche casuali alle immagini originali. Il processo avviene dinamicamente durante l'addestramento: a ogni epoca la rete vede versioni leggermente diverse delle stesse immagini, favorendo l'apprendimento di feature più robuste.
Sono state impostate rotazioni fino a 20°, traslazioni del 10% e zoom del 10% per simulare le naturali variazioni di posizionamento del paziente nello scanner. Il ribaltamento orizzontale è giustificato dalla simmetria bilaterale del cervello.

```{r model2-augmentation}
train.datagen.aug <- image_data_generator(
  rescale = 1/255,
  rotation_range = 20,
  width_shift_range = 0.1,
  height_shift_range = 0.1,
  shear_range = 0.1,
  zoom_range = 0.1,
  horizontal_flip = TRUE,
  fill_mode = "nearest"
)

train.generator.aug <- flow_images_from_directory(
  file.path(path.new.dir, "train"),
  train.datagen.aug, 
  target_size = c(150, 150),
  batch_size = 20,
  class_mode = "categorical"
)
```

### Architettura
L'architettura è identica al Modello 1, con l'aggiunta di un layer di dropout (rate 0.3) prima del classificatore finale.
```{r model2-architecture}
model2 <- keras_model_sequential() %>%
  
  layer_conv_2d(filters = 32, kernel_size = c(3, 3), activation = "relu", 
                input_shape = c(150, 150, 3)) %>%
  layer_max_pooling_2d(pool_size = c(2, 2)) %>%
  
  layer_conv_2d(filters = 64, kernel_size = c(3, 3), activation = "relu") %>%
  layer_max_pooling_2d(pool_size = c(2, 2)) %>%
  
  layer_conv_2d(filters = 128, kernel_size = c(3, 3), activation = "relu") %>%
  layer_max_pooling_2d(pool_size = c(2, 2)) %>%
  
  layer_conv_2d(filters = 128, kernel_size = c(3, 3), activation = "relu") %>%
  layer_max_pooling_2d(pool_size = c(2, 2)) %>%
  
  layer_flatten() %>%
  layer_dropout(rate = 0.3) %>%
  layer_dense(units = 512, activation = "relu") %>%
  
  layer_dense(units = 3, activation = "softmax")

summary(model2)

model2 %>% compile(
  loss = "categorical_crossentropy",
  # Ottimizzatore per Mac M1
  optimizer = tf$keras$optimizers$legacy$RMSprop(learning_rate = 1e-4),
  metrics = c("acc")
)
```

Il modello viene addestrato per 60 epoche, il doppio rispetto al modello base, poiché la data augmentation introduce maggiore variabilità nei dati e richiede più tempo per convergere. I class weights vengono passati alla funzione di training per bilanciare la loss.

```{r model2-train, eval=FALSE}
history2 <- model2 %>% fit(
  train.generator.aug,
  steps_per_epoch = steps.train,
  epochs = 60,
  validation_data = validation.generator,
  validation_steps = steps.val,
  class_weight = class.weights
)
```

```{r model2-load, include=FALSE}
path_model_2 <- here("Modelli salvati", "modello2.h5")
path_hist_2  <- here("Modelli salvati", "training2.rds")

model2 <- load_model_hdf5(path_model_2)
history2 <- readRDS(path_hist_2)
```

### Valutazione

```{r model2-plot}
plot(history2)
```

Le curve di apprendimento mostrano un comportamento sano: training e validation loss decrescono in parallelo senza divergere, segno che l'overfitting è stato efficacemente contrastato. La validation loss risulta inferiore alla training loss, fenomeno atteso in presenza di dropout, che viene applicato solo durante il training rendendo la fase di apprendimento più "difficile" rispetto alla validazione.

```{r model2-eval}
model2 %>% evaluate(test.generator)

prob2 <- model2 %>% predict(test.generator)
predict2 <- apply(prob2, 1, which.max) - 1 
confusionMatrix(as.factor(predict2), as.factor(true))
```

L'accuracy sul test set è del 92.6%, leggermente inferiore al Modello 1 (94.1%). Tuttavia, l'analisi della matrice di confusione rivela un netto miglioramento nella gestione delle classi sbilanciate: la sensitivity per i meningiomi sale dal 85.9% al 93.4%, allineandosi a quella delle altre classi. La Balanced Accuracy risulta ora uniforme tra le tre categorie (93-97%), confermando che i class weights hanno corretto il bias verso la classe maggioritaria.

Una piccola perdita in termini di accuracy in favore di un miglior bilanciamento tra classi è accettabile in un contesto clinico, dove è preferibile un modello equo piuttosto che uno che sacrifica le classi minoritarie.

## Modello 3: VGG16 feature extraction

Il transfer learning permette di sfruttare le features apprese da VGG16 su ImageNet (milioni di immagini e miglia di classi). In questa configurazione, la base convoluzionale viene utilizzata come estrattore di features, congelandone i pesi, per poi passarle a un nuovo classificatore addestrato da zero.

### Estrazione delle features
Si carica la base della rete VGG16 già addestrata e si rimuove la parte finale (il top) perché useremo la rete solo per elaborare le immagini, non per la classificazione finale.

```{r model3-base}
conv.base <- application_vgg16(
  weights = "imagenet",
  include_top = FALSE,
  input_shape = c(150, 150, 3)
)
```

Si definisce una funzione che passa le immagini del MRI attraverso la VGG16. In questa fase la rete non viene addestrata, ma si limita a trasformare ogni immagine in una mappa di features numeriche che descrivono il contenuto visivo.
```{r model3-extraction}
extract.features <- function(directory, sample.count) {
  
  datagen <- image_data_generator(rescale = 1/255)
  batch_size <- 20
  
  generator <- flow_images_from_directory(
    directory = directory,
    generator = datagen,
    target_size = c(150, 150),
    batch_size = batch_size,
    class_mode = "categorical",
    shuffle = FALSE
  )
  
  n.images <- generator$n
  if (n.images != sample.count) {
    sample.count <- n.images
  }
  
  features <- array(0, dim = c(sample.count, 4, 4, 512))
  labels <- array(0, dim = c(sample.count, 3)) 
  i <- 0
  
  while(TRUE) {
    batch <- generator_next(generator)
    inputs.batch <- batch[[1]]
    labels.batch <- batch[[2]]
    
    features.batch <- conv.base %>% predict(inputs.batch, verbose = 0)
    batch.size.true <- dim(features.batch)[1]
    
    start.index <- (i * batch_size) + 1
    end.index   <- (i * batch_size) + batch.size.true
    
    features[start.index:end.index,,,] <- features.batch
    labels[start.index:end.index, ]    <- labels.batch
    
    i <- i + 1
    if (i * batch_size >= sample.count) break 
  }
  
  return(list(features = features, labels = labels))
}

train.data <- extract.features(file.path(path.new.dir, "train"), 5000) 
val.data <- extract.features(file.path(path.new.dir, "validation"), 1000)
test.data <- extract.features(file.path(path.new.dir, "test"), 1000)
```

Le caratteristiche estratte dalla VGG16 sono tridimensionali. In questo passaggio vengono appiattite in un unico vettore per renderle leggibili dal classificatore che verrà costruito successivamente.
```{r model3-reshape}
reshape.features <- function(features) {
  array_reshape(features, dim = c(nrow(features), 4 * 4 * 512))
}

train.feat <- reshape.features(train.data$features)
val.feat   <- reshape.features(val.data$features)
test.feat  <- reshape.features(test.data$features)

train.labels <- train.data$labels
val.labels   <- val.data$labels
test.labels  <- test.data$labels
```

### Classificatore
si definisce l'architettura del classificatore finale è costituita da un fully connected layer seguito da un Dropout al 50% per la regolarizzazione. Il modello conta complessivamente 2,098,179 parametri addestrabili.
```{r model3-classifier}
model3 <- keras_model_sequential() %>%
  layer_dense(units = 256, activation = "relu", input_shape = 4 * 4 * 512) %>%
  layer_dropout(rate = 0.5) %>%
  layer_dense(units = 3, activation = "softmax") 

summary(model3)

model3 %>% compile(
  # Ottimizzatore per Mac M1
  optimizer = tf$keras$optimizers$legacy$RMSprop(learning_rate = 2e-5),
  loss = "categorical_crossentropy",
  metrics = c("acc")
)
```

```{r model3-train, eval=FALSE}
history3 <- model3 %>% fit(
  train.feat, train.labels,
  epochs = 30,
  batch_size = 20,
  validation_data = list(val.feat, val.labels)
)
```

```{r model3-load, include=FALSE}
path_model_3 <- here("Modelli salvati", "modello3.h5")
path_hist_3  <- here("Modelli salvati", "training3.rds")

model3 <- load_model_hdf5(path_model_3)
history3 <- readRDS(path_hist_3)
```

### Valutazione

```{r model3-plot}
plot(history3)
```

Le curve di apprendimento mostrano un andamento molto stabile e parallelo, sintomo di una buona capacità di generalizzazione e assenza di overfitting significativo. Tuttavia, si osserva che la validation accuracy raggiunge rapidamente un livello stabile intorno al 87% già dopo le prime 10 epoche, faticando a migliorare ulteriormente. Le performance finali risultano quindi inferiori rispetto ai modelli precedenti.

```{r model3-eval}
model3 %>% evaluate(test.feat, test.labels)

prob3 <- model3 %>% predict(test.feat)
true3 <- apply(test.data$labels, 1, which.max) - 1
predict3 <- apply(prob3, 1, which.max) - 1 
confusionMatrix(as.factor(predict3), as.factor(true3))
```

Nonostante una accuratezza complessiva del 91.1%, l'analisi per classe evidenzia una criticità specifica legata alla distinzione tra tumori. Mentre il modello eccelle nell'identificare i tumori pituitari (precision 97.1%), soffre notevolmente sulla classe 1 (Meningioma), che presenta la precision più bassa, pari al 78.5%. Questo calo è dovuto all'alto numero di falsi positivi: il modello ha classificato erroneamente come Meningiomi ben 23 immagini che in realtà erano Gliomi (classe 0). I filtri di VGG16, addestrati su immagini naturali, non sono ottimali per MRI cerebrali.

## Modello 4: VGG16 fine-tuning

Il fine-tuning rappresenta una tecnica complementare alla feature extraction. Mentre l'approccio precedente utilizzava la VGG16 come un estrattore statico, questa tecnica consiste nello scongelare l'ultimo blocco convoluzionale (block5) per renderlo addestrabile insieme al classificatore. La logica sottostante si basa sulla gerarchia delle feature: i primi layer della rete rilevano caratteristiche universali (bordi, texture semplici) che sono valide anche per le MRI, mentre gli ultimi layer codificano concetti complessi e specifici (es. occhi, ruote). Riadattando solo questi ultimi filtri al nuovo dominio, permettiamo al modello di specializzarsi nel riconoscimento dei tessuti tumorali senza perdere la robustezza appresa su ImageNet.

### Configurazione
L'intera base convoluzionale viene congelata per preservare le caratteristiche apprese su ImageNet. Successivamente, viene scongelato solo l'ultimo blocco (Block 5), rendendolo modificabile durante l'addestramento.
```{r model4-setup}
conv.base <- application_vgg16(
  weights = "imagenet",
  include_top = FALSE,
  input_shape = c(150, 150, 3)
)

# Congelamento di tutti i layer
freeze_weights(conv.base)

# Sblocco selettivo dell'ultimo blocco
unfreeze_weights(conv.base, from = "block5_conv1")
```

Poiché nel fine-tuning la complessità del modello viene aumentata rendendo addestrabili anche gli strati convoluzionali del block5, il rischio di overfitting aumenta sensibilmente rispetto alla semplice feature extraction.
Pertanto, è indispensabile reintrodurre la data augmentation in questa fase in modo da impedire al modello di memorizzare gli esempi di training, costringendolo invece ad apprendere caratteristiche strutturali robuste e generalizzabili.
```{r model4-augmentation}
train.datagen.ft <- image_data_generator(
  rescale = 1/255,
  rotation_range = 20,
  width_shift_range = 0.1,
  height_shift_range = 0.1,
  shear_range = 0.1,
  zoom_range = 0.1,
  horizontal_flip = TRUE,
  fill_mode = "nearest"
)

train.generator.ft <- flow_images_from_directory(
  file.path(path.new.dir, "train"),
  train.datagen.ft,
  target_size = c(150, 150),
  batch_size = 20,
  class_mode = "categorical"
)
```

### Architettura
L'architettura combina la base VGG16 (con il block5 sbloccato) con un classificatore personalizzato. Viene inserito un Dropout al 50% per prevenire l'overfitting sui nuovi dati. Nell'ottimizzatore RMSprop viene scelto un learning rate pari a $\lambda = 10^{-5}$, questo valore permette di adattare delicatamente i pesi pre-esistenti alle nuove immagini MRI senza stravolgere i pattern appresi su ImageNet. L'elevato numero di parametri (circa 16.8 milioni complessivi, di cui oltre 9 milioni allenabili) e una maggior profondità della rete hanno comportato un costo computazionale notevole rispetto ai modelli precedenti.
```{r model4-architecture}
model.ft <- keras_model_sequential() %>%
  conv.base %>%
  layer_flatten() %>%
  layer_dropout(rate = 0.5) %>%
  layer_dense(units = 256, activation = "relu") %>%
  layer_dense(units = 3, activation = "softmax")

summary(model.ft)

model.ft %>% compile(
  loss = "categorical_crossentropy",
  optimizer = tf$keras$optimizers$legacy$RMSprop(learning_rate = 1e-5),
  metrics = c("acc")
)
```

```{r model4-train, eval=FALSE}
history.ft <- model.ft %>% fit(
  train.generator.ft,
  steps_per_epoch = ceiling(train.generator.ft$n / 20),
  epochs = 30,
  validation_data = validation.generator,
  validation_steps = ceiling(validation.generator$n / 20)
)

save_model_hdf5(model.ft, "modello_ft.h5")
saveRDS(history.ft, "training_ft.rds")
```

```{r model4-load, include=FALSE}
path_model_ft <- here("Modelli salvati", "modello_ft.h5")
path_hist_ft  <- here("Modelli salvati", "training_ft.rds")

model.ft <- load_model_hdf5(path_model_ft)
history.ft <- readRDS(path_hist_ft)
```

### Valutazione

```{r model4-plot}
plot(history.ft)
```

Le curve di training e validation sono praticamente sovrapposte, indicando un'ottima generalizzazione senza overfitting, confermando l'efficacia delle strategie di regolarizzazione adottate.

```{r model4-eval}
model.ft %>% evaluate(test.generator)

prob.ft <- model.ft %>% predict(test.generator)
pred.ft <- apply(prob.ft, 1, which.max) - 1

confusionMatrix(factor(pred.ft), factor(true))
```
Il modello basato sul fine-tuning si conferma come il più performante tra i modelli addestrati, raggiungendo un'accuratezza del 95%. Per tutte le classi si registra una precision superiore al 92% e una sensitivity che supera il 90%. In un contesto clinico reale questo miglioramento è cruciale: il modello dimostra di saper discriminare efficacemente le classi riducendo gli errori diagnostici, fornendo così un valido strumento di supporto per il personale sanitario.

Il modello basato sul fine-tuning, avendo dimostrato le migliori capacità di generalizzazione, è stato selezionato per l'integrazione con la conformal prediction.

# Conformal Prediction

La conformal prediction fornisce garanzie probabilistiche sulla copertura delle predizioni. Dato un livello di confidenza $1 - \alpha$, il metodo costruisce dei prediction sets che contengono la vera classe con probabilità almeno $1 - \alpha$. In ambito medico, questo approccio è particolarmente prezioso, piuttosto che fornire una classificazione assoluta (che potrebbe essere errata nei casi clinici più ambigui), il modello restituisce un insieme di possibili diagnosi. Questo permette di gestire esplicitamente l'incertezza e ridurre drasticamente il rischio di errori diagnostici.

## Calibrazione
Per implementare la split conformal prediction, si considera il validation set come calibration set. L'obiettivo è calcolare i non-conformity scores, ovvero una misura di quanto ogni immagine sia difficile da classificare per il modello.
In questo caso, il punteggio è definito come $1 - \hat{f}(x)_y$, dove $\hat{f}(x)_y$ è la probabilità assegnata dal modello alla classe corretta.
```{r conformal-setup}
val.gen.conf <- flow_images_from_directory(
  file.path(path.new.dir, "validation"),
  image_data_generator(rescale = 1/255),
  target_size = c(150, 150),
  batch_size = 20,
  class_mode = "categorical",
  shuffle = FALSE
)
```

Fissato un livello di errore $\alpha = 0.05$, si impone che nel 95% dei casi la diagnosi corretta sia inclusa nel prediction set restituito dal modello.
```{r conformal-calibration, results= "hide"}
alpha <- 0.05  # Livello di confidenza 95%

val.probs <- predict(model.ft, val.gen.conf, verbose = 1)
val.true <- val.gen.conf$classes

test.probs <- prob.ft
test.true <- true

# Non-conformity scores sul calibration set
val.true.idx <- cbind(1:nrow(val.probs), val.true + 1)
prob.true.class <- val.probs[val.true.idx]
scores <- 1 - prob.true.class
```

## Calcolo della Soglia
```{r conformal-threshold}
n <- length(scores)
q.level <- ceiling((n + 1) * (1 - alpha)) / n
q.hat <- quantile(scores, probs = q.level)
q.hat; 1-q.hat
```
Il quantile $\frac{\lceil(1-\alpha)(n_{cal}+1)\rceil}{n_{cal}}$ della distribuzione degli score calcolati in precedenza risulta pari a $\hat{q} = 0.6501$. Di conseguenza, affinché una classe sia inclusa nel prediction set, deve presentare una probabilità predetta dal modello almeno pari a $1-\hat q = 0.3499$.

## Prediction Sets
Visualizzazione di 20 prediction sets casuali
```{r conformal-sets}
prediction.sets <- test.probs >= (1 - q.hat)

class.names <- c("Glioma", "Meningioma", "Pituitary")

# Visualizzare 20 prediction set casuali
set.seed(2026)
random_indices <- sample(1:length(test.true), 20)

for (i in random_indices) {
  classes.in.set <- class.names[prediction.sets[i, ]]
  cat(sprintf("Img %d (Vero: %s) -> Prediction Set: {%s}\n", 
              i, class.names[test.true[i]+1], paste(classes.in.set, collapse = ", ")))
}
```

## Metriche
La robustezza del metodo viene valutata confrontando la coverage empirica con la soglia teorica del 95% e calcolando la dimensione media dei prediction sets ottenuti.
```{r conformal-metrics}
# Coverage empirico
covered <- prediction.sets[cbind(1:nrow(test.probs), test.true + 1)]
empirical.coverage <- mean(covered)
round(empirical.coverage, 4)

# Average set size
avg.set.size <- mean(rowSums(prediction.sets))
round(avg.set.size, 3)
```

La coverage empirica (95.86%) rispetta la garanzia teorica del 95%. L'average set size di 1.026 indica che il modello è molto sicuro delle sue predizioni: nella quasi totalità dei casi il prediction set contiene una sola classe.

# Conclusioni

Il presente lavoro ha confrontato quattro architetture di reti neurali convoluzionali per la classificazione di tumori cerebrali, evidenziando un compromesso tra complessità del modello, capacità di generalizzazione e gestione dello sbilanciamento delle classi.

Il Modello 1 (CNN base) ha raggiunto un'accuracy del 94.1% ma con marcato overfitting e scarsa sensitivity sulla classe minoritaria. L'introduzione di tecniche di regolarizzazione nel modello 2 ha risolto entrambi i problemi, bilanciando le performance tra le classi a fronte di una lieve riduzione dell'accuracy complessiva (92.6%). I modelli basati su transfer learning hanno confermato l'efficacia di VGG16 come estrattore di feature: la feature extraction (modello 3) ha ottenuto l'91.1%, mentre il fine-tuning (modello 4) ha raggiunto il 95% con ottime curve di generalizzazione.

L'applicazione della split conformal prediction al modello migliore ha fornito una quantificazione dell'incertezza, con coverage empirica del 95.86% e prediction set di dimensione media pari a 1.03. Questo risultato indica che il modello produce predizioni calibrate e affidabili, requisito fondamentale per un eventuale impiego come strumento di supporto diagnostico.

Il lavoro presenta alcune limitazioni: il dataset, pur essendo di buona qualità, è di dimensioni contenute (3064 immagini) e l'addestramento è stato condotto con risorse computazionali limitate. Un training più prolungato e l'esplorazione di architetture diverse potrebbero migliorare ulteriormente le performance.
